# ë™ì˜ìƒ ë¶„ì„ ê°€ì´ë“œ

ë™ì˜ìƒì„ ì¥ë©´ë³„ë¡œ ë¶„ì„í•˜ì—¬ í¸ì§‘ì— í™œìš©í•˜ëŠ” ì™„ì „í•œ ê°€ì´ë“œì…ë‹ˆë‹¤.

## ğŸ“¹ ì‹œìŠ¤í…œ ê°œìš”

```
ë™ì˜ìƒ íŒŒì¼
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ì‹œê° (ì˜ìƒ) â”‚     â”‚ ì²­ê° (ìŒì„±)  â”‚
â”‚  ffmpeg     â”‚     â”‚   Whisper    â”‚
â”‚  â†“ ì´ë¯¸ì§€   â”‚     â”‚   â†“ í…ìŠ¤íŠ¸   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“                      â†“
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
       ë©€í‹°ëª¨ë‹¬ LLM ë¶„ì„
               â†“
       ì¢…í•© ë¶„ì„ ê²°ê³¼
               â†“
    ë™ì˜ìƒ í¸ì§‘ ë©”íƒ€ë°ì´í„°
```

## ğŸ¯ ì£¼ìš” ê¸°ëŠ¥

1. **ì¥ë©´ ì¶”ì¶œ**: ffmpegë¡œ ë™ì˜ìƒ â†’ ì´ë¯¸ì§€
2. **ìŒì„± ì¸ì‹**: Whisperë¡œ ìŒì„± â†’ í…ìŠ¤íŠ¸
3. **ì‹œê° ë¶„ì„**: ë©€í‹°ëª¨ë‹¬ LLMìœ¼ë¡œ ì´ë¯¸ì§€ ë¶„ì„
4. **ì¢…í•© ë¶„ì„**: ì´ë¯¸ì§€ + ëŒ€ì‚¬ í†µí•© ë¶„ì„
5. **í¸ì§‘ ê°€ì´ë“œ**: ìë™ í¸ì§‘ ì œì•ˆ ìƒì„±

---

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### 1ë‹¨ê³„: ì˜ì¡´ì„± ì„¤ì¹˜

```bash
# ë¹„ë””ì˜¤ ì²˜ë¦¬ íŒ¨í‚¤ì§€
pip install -r requirements_video.txt

# ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ (ì„ íƒ, ê³ ê¸‰ ë¶„ì„ìš©)
pip install -r requirements_multimodal.txt

# ffmpeg ì„¤ì¹˜ (í•„ìˆ˜)
# Mac
brew install ffmpeg

# Ubuntu
sudo apt install ffmpeg

# í™•ì¸
ffmpeg -version
```

### 2ë‹¨ê³„: ê¸°ë³¸ ë¶„ì„ (STTë§Œ)

```bash
python src/video_analyzer.py your_video.mp4
```

ì¶œë ¥:
- `outputs/video_analysis/frames/` - ì¶”ì¶œëœ ì´ë¯¸ì§€ë“¤
- `outputs/video_analysis/audio.wav` - ì¶”ì¶œëœ ì˜¤ë””ì˜¤
- `outputs/video_analysis/analysis_result.json` - ë¶„ì„ ê²°ê³¼
- `outputs/video_analysis/editing_guide.txt` - í¸ì§‘ ê°€ì´ë“œ

### 3ë‹¨ê³„: ê³ ê¸‰ ë¶„ì„ (ë©€í‹°ëª¨ë‹¬ í¬í•¨)

```bash
python src/video_analyzer.py your_video.mp4 \
    --use_multimodal \
    --multimodal_model_path "llava-hf/llava-1.5-7b-hf"
```

---

## ğŸ“ ìƒì„¸ ì‚¬ìš©ë²•

### í”„ë ˆì„ ì¶”ì¶œ ë°©ì‹

#### ë°©ë²• 1: ì¼ì • ê°„ê²©ìœ¼ë¡œ ì¶”ì¶œ (ê¶Œì¥)

```bash
# 2ì´ˆë§ˆë‹¤ í”„ë ˆì„ ì¶”ì¶œ
python src/video_analyzer.py video.mp4 --interval 2.0

# 1ì´ˆë§ˆë‹¤ (ë” ìì„¸í•œ ë¶„ì„)
python src/video_analyzer.py video.mp4 --interval 1.0

# 5ì´ˆë§ˆë‹¤ (ë¹ ë¥¸ ë¶„ì„)
python src/video_analyzer.py video.mp4 --interval 5.0
```

#### ë°©ë²• 2: ì¥ë©´ ë³€í™” ê°ì§€

```python
from src.video_processing import VideoProcessor

processor = VideoProcessor("video.mp4")

# ì¥ë©´ ë³€í™” ê°ì§€ë¡œ ì¶”ì¶œ
frames = processor.extract_frames_by_scene(
    threshold=30.0,  # ë†’ì„ìˆ˜ë¡ í° ë³€í™”ë§Œ ê°ì§€
    min_scene_duration=1.0  # ìµœì†Œ ì¥ë©´ ê¸¸ì´
)
```

### STT (Speech-to-Text)

```python
from src.stt_utils import STTProcessor

# STT í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”
stt = STTProcessor(
    model_size="base",  # tiny, base, small, medium, large
    language="ko"  # í•œêµ­ì–´
)

# ìŒì„± ì¸ì‹
result = stt.transcribe("audio.wav")

print(f"ì „ì²´ í…ìŠ¤íŠ¸: {result['text']}")

# íƒ€ì„ìŠ¤íƒ¬í”„ë³„ ëŒ€ì‚¬
for seg in result['segments']:
    print(f"{seg['start']:.2f}s - {seg['end']:.2f}s: {seg['text']}")
```

### ë©€í‹°ëª¨ë‹¬ ë¶„ì„

```python
from src.multimodal_utils import MultiModalModel

# ëª¨ë¸ ë¡œë”©
model = MultiModalModel(
    model_name="llava-hf/llava-1.5-7b-hf",
    model_type="llava"
)

# ì´ë¯¸ì§€ ë¶„ì„
description = model.generate_from_image(
    "frame_001.jpg",
    "ì´ ì¥ë©´ì„ ì„¤ëª…í•˜ì„¸ìš”"
)

# ê°ì • ë¶„ì„
mood = model.generate_from_image(
    "frame_001.jpg",
    "ì´ ì¥ë©´ì˜ ë¶„ìœ„ê¸°ëŠ”?"
)
```

---

## ğŸ¬ ì™„ì „í•œ íŒŒì´í”„ë¼ì¸

### Python ìŠ¤í¬ë¦½íŠ¸

```python
# analyze_video.py
from src.video_analyzer import VideoAnalyzer
from src.multimodal_utils import MultiModalModel

# 1. ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ ë¡œë”© (ì„ íƒ)
multimodal_model = MultiModalModel(
    model_name="llava-hf/llava-1.5-7b-hf",
    model_type="llava"
)

# 2. ë¶„ì„ê¸° ì´ˆê¸°í™”
analyzer = VideoAnalyzer(
    video_path="my_video.mp4",
    multimodal_model=multimodal_model,
    output_dir="outputs/my_analysis"
)

# 3. ì „ì²´ ë¶„ì„ ì‹¤í–‰
results = analyzer.analyze_full_pipeline(
    extract_method="interval",
    interval_seconds=2.0
)

# 4. í¸ì§‘ ê°€ì´ë“œ ìƒì„±
guide = analyzer.generate_editing_guide(results)
print(guide)

# 5. í¸ì§‘ ì†Œí”„íŠ¸ì›¨ì–´ìš© ë°ì´í„° export
analyzer.export_for_editing(results, format="json")
```

### ì»¤ë§¨ë“œ ë¼ì¸

```bash
# ê¸°ë³¸ ë¶„ì„ (STT + í”„ë ˆì„ ì¶”ì¶œ)
python src/video_analyzer.py video.mp4

# ë©€í‹°ëª¨ë‹¬ ë¶„ì„ í¬í•¨
python src/video_analyzer.py video.mp4 \
    --use_multimodal \
    --multimodal_model_path "llava-hf/llava-1.5-7b-hf"

# ê°„ê²© ì¡°ì •
python src/video_analyzer.py video.mp4 \
    --interval 1.0 \
    --use_multimodal \
    --multimodal_model_path "outputs/my_model"
```

---

## ğŸ“Š ì¶œë ¥ ë°ì´í„° í˜•ì‹

### analysis_result.json

```json
{
  "video_info": {
    "fps": 30.0,
    "duration": 120.5,
    "width": 1920,
    "height": 1080
  },
  "frames": [
    "outputs/video_analysis/frames/frame_000001_t0.00s.jpg",
    "outputs/video_analysis/frames/frame_000002_t2.00s.jpg"
  ],
  "transcript": {
    "text": "ì „ì²´ ëŒ€ì‚¬...",
    "segments": [
      {
        "start": 0.0,
        "end": 3.5,
        "text": "ì•ˆë…•í•˜ì„¸ìš”, ì—¬ëŸ¬ë¶„"
      }
    ]
  },
  "scenes": [
    {
      "scene_number": 1,
      "timestamp": 0.0,
      "frame_path": "outputs/video_analysis/frames/frame_000001_t0.00s.jpg",
      "dialogue": "ì•ˆë…•í•˜ì„¸ìš”, ì—¬ëŸ¬ë¶„",
      "description": "ë°œí‘œìê°€ í™”ë©´ ì¤‘ì•™ì— ì„œ ìˆìŠµë‹ˆë‹¤...",
      "mood": "ì „ë¬¸ì ì´ê³  ì¹œê·¼í•œ ë¶„ìœ„ê¸°",
      "objects": "ì‚¬ëŒ, ë§ˆì´í¬, í”„ë ˆì  í…Œì´ì…˜ í™”ë©´"
    }
  ]
}
```

### editing_data.json (í¸ì§‘ ì†Œí”„íŠ¸ì›¨ì–´ìš©)

```json
{
  "video": "my_video.mp4",
  "markers": [
    {
      "time": 0.0,
      "label": "Scene 1",
      "description": "ì˜¤í”„ë‹ ì¥ë©´"
    }
  ],
  "scenes": [
    {
      "start": 0.0,
      "description": "ë°œí‘œì ë“±ì¥...",
      "dialogue": "ì•ˆë…•í•˜ì„¸ìš”...",
      "mood": "ì¹œê·¼í•¨"
    }
  ]
}
```

---

## ğŸ¯ ì‹¤ì „ í™œìš© ì‚¬ë¡€

### ì‚¬ë¡€ 1: YouTube ë™ì˜ìƒ ìë™ ì±•í„° ìƒì„±

```python
from src.video_analyzer import VideoAnalyzer

analyzer = VideoAnalyzer("youtube_video.mp4")
results = analyzer.analyze_full_pipeline(interval_seconds=30.0)

# ì±•í„° ìƒì„±
chapters = []
for scene in results['scenes']:
    timestamp = scene['timestamp']
    description = scene.get('description', '')[:50]
    
    # YouTube íƒ€ì„ìŠ¤íƒ¬í”„ í˜•ì‹
    minutes = int(timestamp // 60)
    seconds = int(timestamp % 60)
    chapters.append(f"{minutes:02d}:{seconds:02d} - {description}")

# YouTube ì„¤ëª…ë€ì— ë¶™ì—¬ë„£ê¸°
print("YouTube ì±•í„°:")
for chapter in chapters:
    print(chapter)
```

ì¶œë ¥:
```
00:00 - ì¸íŠ¸ë¡œ: ë°œí‘œì ì†Œê°œ
00:30 - ì£¼ì œ ì„¤ëª…: AI ê¸°ìˆ  ê°œìš”
01:15 - ë°ëª¨ ì‹œì—°: ì‹¤ì œ ì‚¬ìš© ì˜ˆì œ
02:30 - ì§ˆì˜ì‘ë‹µ ì‹œì‘
```

### ì‚¬ë¡€ 2: ê°•ì˜ ë™ì˜ìƒ ìš”ì•½

```python
from src.video_analyzer import VideoAnalyzer
from src.multimodal_utils import MultiModalModel

# ë©€í‹°ëª¨ë‹¬ ëª¨ë¸
mm_model = MultiModalModel(
    model_name="llava-hf/llava-1.5-7b-hf",
    model_type="llava"
)

# ë¶„ì„
analyzer = VideoAnalyzer("lecture.mp4", multimodal_model=mm_model)
results = analyzer.analyze_full_pipeline(interval_seconds=60.0)

# ê°•ì˜ ìš”ì•½ ìƒì„±
summary = {
    'ì œëª©': 'ê°•ì˜ ì œëª©',
    'ê¸¸ì´': f"{results['video_info']['duration'] / 60:.1f}ë¶„",
    'ì£¼ìš” ë‚´ìš©': []
}

for scene in results['scenes']:
    if scene.get('description'):
        summary['ì£¼ìš” ë‚´ìš©'].append({
            'ì‹œê°„': f"{int(scene['timestamp'] // 60)}:{int(scene['timestamp'] % 60):02d}",
            'ë‚´ìš©': scene['description'][:100],
            'ëŒ€ì‚¬': scene.get('dialogue', '')[:100]
        })

print(json.dumps(summary, ensure_ascii=False, indent=2))
```

### ì‚¬ë¡€ 3: ë™ì˜ìƒ í•˜ì´ë¼ì´íŠ¸ ì¶”ì¶œ

```python
from src.video_analyzer import VideoAnalyzer

analyzer = VideoAnalyzer("game_replay.mp4")
results = analyzer.analyze_full_pipeline()

# ì¤‘ìš” ì¥ë©´ ì°¾ê¸° (í° ì†Œë¦¬, ê¸‰ê²©í•œ ë³€í™” ë“±)
highlights = []

for scene in results['scenes']:
    dialogue = scene.get('dialogue', '').lower()
    
    # í™˜í˜¸ ë˜ëŠ” ì¤‘ìš” í‚¤ì›Œë“œ ê°ì§€
    if any(word in dialogue for word in ['ì™€', 'ì˜¤', 'ëŒ€ë°•', 'ê³¨', 'ë“ì ']):
        highlights.append({
            'timestamp': scene['timestamp'],
            'reason': 'ì¤‘ìš” ì´ë²¤íŠ¸ ê°ì§€',
            'dialogue': scene['dialogue']
        })

print(f"í•˜ì´ë¼ì´íŠ¸ {len(highlights)}ê°œ ë°œê²¬:")
for h in highlights:
    print(f"  {h['timestamp']:.2f}s - {h['reason']}: {h['dialogue'][:50]}")
```

### ì‚¬ë¡€ 4: ìë§‰ íŒŒì¼ ìƒì„±

```python
from src.stt_utils import STTProcessor

# STT
stt = STTProcessor(model_size="medium", language="ko")
transcript = stt.transcribe("video_audio.wav")

# SRT ìë§‰ íŒŒì¼ ìƒì„±
def generate_srt(segments, output_file):
    """SRT í˜•ì‹ ìë§‰ ìƒì„±"""
    with open(output_file, 'w', encoding='utf-8') as f:
        for i, seg in enumerate(segments, 1):
            # íƒ€ì„ì½”ë“œ ë³€í™˜
            start = format_srt_time(seg['start'])
            end = format_srt_time(seg['end'])
            
            f.write(f"{i}\n")
            f.write(f"{start} --> {end}\n")
            f.write(f"{seg['text']}\n\n")

def format_srt_time(seconds):
    """ì´ˆë¥¼ SRT íƒ€ì„ì½”ë“œë¡œ ë³€í™˜"""
    hours = int(seconds // 3600)
    minutes = int((seconds % 3600) // 60)
    secs = int(seconds % 60)
    millis = int((seconds % 1) * 1000)
    return f"{hours:02d}:{minutes:02d}:{secs:02d},{millis:03d}"

# ì‚¬ìš©
generate_srt(transcript['segments'], "subtitles.srt")
```

---

## ğŸ› ï¸ ë‹¨ê³„ë³„ ì›Œí¬í”Œë¡œìš°

### ë‹¨ê³„ 1: ë™ì˜ìƒ ì¤€ë¹„

```bash
# ë™ì˜ìƒ íŒŒì¼ ë³µì‚¬
cp ~/Videos/my_video.mp4 data/videos/
```

### ë‹¨ê³„ 2: ê¸°ë³¸ ë¶„ì„ (STTë§Œ)

```bash
python src/video_analyzer.py data/videos/my_video.mp4 \
    --interval 2.0 \
    --output_dir outputs/my_video_analysis
```

**ì¶œë ¥:**
- í”„ë ˆì„ ì´ë¯¸ì§€ë“¤
- ì˜¤ë””ì˜¤ íŒŒì¼
- STT í…ìŠ¤íŠ¸
- ê¸°ë³¸ ë©”íƒ€ë°ì´í„°

**ì†Œìš” ì‹œê°„:** 5-10ë¶„ (5ë¶„ ë™ì˜ìƒ ê¸°ì¤€)

### ë‹¨ê³„ 3: ë©€í‹°ëª¨ë‹¬ ë¶„ì„ (ê³ ê¸‰)

```bash
python src/video_analyzer.py data/videos/my_video.mp4 \
    --interval 2.0 \
    --use_multimodal \
    --multimodal_model_path "llava-hf/llava-1.5-7b-hf"
```

**ì¶œë ¥:**
- ëª¨ë“  ê¸°ë³¸ ë¶„ì„ +
- ê° ì¥ë©´ì˜ ì‹œê°ì  ì„¤ëª…
- ê°ì •/ë¶„ìœ„ê¸° ë¶„ì„
- ê°ì²´ ì¸ì‹ ê²°ê³¼

**ì†Œìš” ì‹œê°„:** 20-30ë¶„ (5ë¶„ ë™ì˜ìƒ, GPU ì‚¬ìš©)

---

## ğŸ“Š í™œìš© ì˜ˆì œ

### ì˜ˆì œ 1: í¸ì§‘ í¬ì¸íŠ¸ ì°¾ê¸°

```python
import json

# ë¶„ì„ ê²°ê³¼ ë¡œë”©
with open('outputs/video_analysis/analysis_result.json', 'r') as f:
    results = json.load(f)

# í¸ì§‘ í¬ì¸íŠ¸ ì¶”ì¶œ
edit_points = []

for scene in results['scenes']:
    timestamp = scene['timestamp']
    
    # ì¥ë©´ ì „í™˜ í¬ì¸íŠ¸
    edit_points.append({
        'time': timestamp,
        'type': 'scene_change',
        'frame': scene['frame_path']
    })
    
    # ëŒ€ì‚¬ ì‹œì‘ í¬ì¸íŠ¸
    if scene.get('dialogue') and scene['dialogue'].strip():
        edit_points.append({
            'time': timestamp,
            'type': 'dialogue_start',
            'text': scene['dialogue'][:50]
        })

# Premiere Pro XML ìƒì„±
print("í¸ì§‘ í¬ì¸íŠ¸:")
for point in edit_points:
    print(f"{point['time']:.2f}s - {point['type']}")
```

### ì˜ˆì œ 2: ì¥ë©´ë³„ íƒœê·¸ ìƒì„±

```python
# ë¶„ì„ ê²°ê³¼ì—ì„œ íƒœê·¸ ìƒì„±
tags_by_scene = []

for scene in results['scenes']:
    tags = []
    
    # ê°ì²´ ê¸°ë°˜ íƒœê·¸
    if scene.get('objects'):
        objects = scene['objects'].lower()
        if 'ì‚¬ëŒ' in objects:
            tags.append('people')
        if 'ìì—°' in objects or 'í’ê²½' in objects:
            tags.append('nature')
        if 'ì‹¤ë‚´' in objects:
            tags.append('indoor')
    
    # ë¶„ìœ„ê¸° ê¸°ë°˜ íƒœê·¸
    if scene.get('mood'):
        mood = scene['mood'].lower()
        if 'ë°' in mood or 'ì¦ê±°' in mood:
            tags.append('positive')
        if 'ì–´ë‘' in mood or 'ìŠ¬' in mood:
            tags.append('serious')
    
    tags_by_scene.append({
        'timestamp': scene['timestamp'],
        'tags': tags
    })

# í™œìš©: íŠ¹ì • íƒœê·¸ê°€ ìˆëŠ” ì¥ë©´ë§Œ í¸ì§‘
nature_scenes = [
    s for s in tags_by_scene
    if 'nature' in s['tags']
]
```

### ì˜ˆì œ 3: ìë™ í•˜ì´ë¼ì´íŠ¸ ì˜ìƒ ìƒì„±

```python
from moviepy.editor import VideoFileClip, concatenate_videoclips

# ë¶„ì„ ê²°ê³¼ì—ì„œ í•˜ì´ë¼ì´íŠ¸ ì°¾ê¸°
def find_highlights(results):
    """ì¤‘ìš” ì¥ë©´ ì°¾ê¸°"""
    highlights = []
    
    for scene in results['scenes']:
        score = 0
        
        # ëŒ€ì‚¬ê°€ ìˆìœ¼ë©´ +1
        if scene.get('dialogue'):
            score += 1
        
        # ê°ì • í‚¤ì›Œë“œ ìˆìœ¼ë©´ +2
        dialogue = scene.get('dialogue', '').lower()
        if any(word in dialogue for word in ['ì¤‘ìš”', 'í•µì‹¬', 'í¬ì¸íŠ¸', 'ì™€', 'ëŒ€ë°•']):
            score += 2
        
        # ë¶„ìœ„ê¸°ê°€ ì—­ë™ì ì´ë©´ +1
        mood = scene.get('mood', '').lower()
        if any(word in mood for word in ['ì—­ë™', 'í¥ë¯¸', 'ê·¹ì ']):
            score += 1
        
        if score >= 2:
            highlights.append({
                'timestamp': scene['timestamp'],
                'score': score,
                'duration': 3.0  # 3ì´ˆ í´ë¦½
            })
    
    return highlights

# í•˜ì´ë¼ì´íŠ¸ í´ë¦½ ìƒì„±
video = VideoFileClip("my_video.mp4")
highlights = find_highlights(results)

clips = []
for h in highlights:
    start = h['timestamp']
    end = start + h['duration']
    clip = video.subclip(start, end)
    clips.append(clip)

# í•˜ì´ë¼ì´íŠ¸ ì˜ìƒ í•©ì¹˜ê¸°
if clips:
    final = concatenate_videoclips(clips)
    final.write_videofile("highlights.mp4")
    print(f"í•˜ì´ë¼ì´íŠ¸ ì˜ìƒ ìƒì„± ì™„ë£Œ: highlights.mp4")
```

---

## ğŸ¨ ê³ ê¸‰ ê¸°ëŠ¥

### ì»¤ìŠ¤í…€ ë¶„ì„ íŒŒì´í”„ë¼ì¸

```python
from src.video_processing import VideoProcessor
from src.stt_utils import STTProcessor
from src.multimodal_utils import MultiModalModel

class CustomVideoAnalyzer:
    """ì»¤ìŠ¤í…€ ë™ì˜ìƒ ë¶„ì„ê¸°"""
    
    def __init__(self, video_path: str):
        self.video_path = video_path
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.video_proc = VideoProcessor(video_path)
        self.stt = STTProcessor(model_size="base", language="ko")
        self.vision_model = MultiModalModel(
            model_name="llava-hf/llava-1.5-7b-hf",
            model_type="llava"
        )
    
    def analyze_for_editing(self):
        """í¸ì§‘ìš© ì¢…í•© ë¶„ì„"""
        
        # 1. ì¥ë©´ ê°ì§€ë¡œ í”„ë ˆì„ ì¶”ì¶œ
        frames = self.video_proc.extract_frames_by_scene()
        
        # 2. ì˜¤ë””ì˜¤ ì¶”ì¶œ ë° STT
        audio = self.video_proc.extract_audio()
        transcript = self.stt.transcribe(audio)
        
        # 3. ê° ì¥ë©´ ë¶„ì„
        analysis = []
        
        for frame_path in frames:
            timestamp = self.video_proc.get_frame_timestamp(
                os.path.basename(frame_path)
            )
            
            # ì´ë¯¸ì§€ ë¶„ì„
            scene_desc = self.vision_model.generate_from_image(
                frame_path,
                "ì´ ì¥ë©´ì—ì„œ ì¼ì–´ë‚˜ëŠ” ì¼ì„ ì„¤ëª…í•˜ì„¸ìš”"
            )
            
            # í•´ë‹¹ ì‹œê°„ ëŒ€ì‚¬
            dialogue = self.stt.get_transcript_at_time(
                transcript['segments'],
                timestamp,
                context_window=5.0
            )
            
            # í¸ì§‘ ì œì•ˆ
            edit_suggestion = self._suggest_edit(scene_desc, dialogue)
            
            analysis.append({
                'timestamp': timestamp,
                'visual': scene_desc,
                'audio': dialogue,
                'edit_suggestion': edit_suggestion
            })
        
        return analysis
    
    def _suggest_edit(self, visual: str, audio: str) -> str:
        """ì¥ë©´ ê¸°ë°˜ í¸ì§‘ ì œì•ˆ"""
        suggestions = []
        
        if not audio.strip():
            suggestions.append("BGM ì¶”ê°€ ê¶Œì¥")
        
        if 'ì‚¬ëŒ' in visual and len(audio.split()) < 3:
            suggestions.append("í´ë¡œì¦ˆì—… ìƒ· ê³ ë ¤")
        
        if 'í’ê²½' in visual or 'ìì—°' in visual:
            suggestions.append("ì™€ì´ë“œ ìƒ· ìœ ì§€")
        
        return ' | '.join(suggestions) if suggestions else "ê¸°ë³¸ í¸ì§‘"
```

---

## âš™ï¸ ì„¤ì • ì˜µì…˜

### Whisper ëª¨ë¸ í¬ê¸° ì„ íƒ

| ëª¨ë¸ | í¬ê¸° | ì†ë„ | ì •í™•ë„ | ìš©ë„ |
|------|------|------|--------|------|
| tiny | 39M | â­â­â­â­â­ | â­â­ | ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ |
| base | 74M | â­â­â­â­ | â­â­â­ | **ê¶Œì¥** |
| small | 244M | â­â­â­ | â­â­â­â­ | ê· í˜• |
| medium | 769M | â­â­ | â­â­â­â­â­ | ê³ í’ˆì§ˆ |
| large | 1550M | â­ | â­â­â­â­â­ | ìµœê³  í’ˆì§ˆ |

### í”„ë ˆì„ ì¶”ì¶œ ê°„ê²©

| ê°„ê²© | ìš©ë„ | í”„ë ˆì„ ìˆ˜ (5ë¶„ ì˜ìƒ) |
|------|------|---------------------|
| 0.5ì´ˆ | ìƒì„¸ ë¶„ì„ | 600ê°œ |
| 1.0ì´ˆ | ì¼ë°˜ ë¶„ì„ | 300ê°œ |
| 2.0ì´ˆ | **ê¶Œì¥** | 150ê°œ |
| 5.0ì´ˆ | ë¹ ë¥¸ ë¶„ì„ | 60ê°œ |
| 10ì´ˆ | ê°œìš”ë§Œ | 30ê°œ |

---

## ğŸ“¦ ì „ì²´ ì‹œìŠ¤í…œ êµ¬ì¡°

```
ë™ì˜ìƒ ë¶„ì„ ì‹œìŠ¤í…œ
â”œâ”€â”€ video_processing.py    # ffmpeg ê¸°ë°˜ ì²˜ë¦¬
â”œâ”€â”€ stt_utils.py          # Whisper STT
â”œâ”€â”€ multimodal_utils.py   # ë©€í‹°ëª¨ë‹¬ LLM
â””â”€â”€ video_analyzer.py     # í†µí•© ë¶„ì„ê¸°

ì…ë ¥:
â”œâ”€â”€ ë™ì˜ìƒ íŒŒì¼ (.mp4, .avi, .mov ë“±)

ì¶œë ¥:
â”œâ”€â”€ frames/               # ì¶”ì¶œëœ ì´ë¯¸ì§€
â”œâ”€â”€ audio.wav            # ì¶”ì¶œëœ ì˜¤ë””ì˜¤
â”œâ”€â”€ analysis_result.json # ì¢…í•© ë¶„ì„ ê²°ê³¼
â”œâ”€â”€ editing_guide.txt    # í¸ì§‘ ê°€ì´ë“œ
â””â”€â”€ editing_data.json    # í¸ì§‘ ì†Œí”„íŠ¸ì›¨ì–´ìš©
```

---

## ğŸ¬ í¸ì§‘ ì†Œí”„íŠ¸ì›¨ì–´ ì—°ë™

### Adobe Premiere Pro

```python
# XML ë§ˆì»¤ ìƒì„±
def generate_premiere_markers(results):
    """Premiere Pro ë§ˆì»¤ XML ìƒì„±"""
    xml = ['<?xml version="1.0" encoding="UTF-8"?>']
    xml.append('<markers>')
    
    for scene in results['scenes']:
        xml.append(f'  <marker time="{scene["timestamp"]}">')
        xml.append(f'    <name>Scene {scene["scene_number"]}</name>')
        xml.append(f'    <comment>{scene.get("description", "")[:100]}</comment>')
        xml.append('  </marker>')
    
    xml.append('</markers>')
    
    return '\n'.join(xml)
```

### DaVinci Resolve

```python
# EDL í˜•ì‹ ìƒì„±
def generate_edl(results):
    """EDL (Edit Decision List) ìƒì„±"""
    edl = ['TITLE: Video Analysis']
    edl.append('FCM: NON-DROP FRAME\n')
    
    for i, scene in enumerate(results['scenes'], 1):
        edl.append(f"{i:03d}  BL  V  C  {format_timecode(scene['timestamp'])}")
        edl.append(f"* FROM CLIP NAME: Scene {i}")
        edl.append(f"* COMMENT: {scene.get('description', '')[:50]}\n")
    
    return '\n'.join(edl)
```

---

## ğŸ’¡ íŒê³¼ íŠ¸ë¦­

### 1. ì„±ëŠ¥ ìµœì í™”

```python
# ê¸´ ë™ì˜ìƒì€ ì²­í¬ë¡œ ë‚˜ëˆ ì„œ ì²˜ë¦¬
def process_long_video(video_path, chunk_duration=300):
    """5ë¶„ì”© ë‚˜ëˆ ì„œ ì²˜ë¦¬"""
    # ffmpegë¡œ ë¶„í• 
    # ê° ì²­í¬ ë¶„ì„
    # ê²°ê³¼ ë³‘í•©
```

### 2. ë°°ì¹˜ ì²˜ë¦¬

```bash
# ì—¬ëŸ¬ ë™ì˜ìƒ ì¼ê´„ ì²˜ë¦¬
for video in data/videos/*.mp4; do
    python src/video_analyzer.py "$video"
done
```

### 3. GPU ë©”ëª¨ë¦¬ ê´€ë¦¬

```python
# ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ì„ í•„ìš”í•  ë•Œë§Œ ë¡œë”©
analyzer = VideoAnalyzer(video_path, multimodal_model=None)
results = analyzer.analyze_full_pipeline()

# ì´ë¯¸ì§€ ë¶„ì„ì´ í•„ìš”í•œ ê²½ìš°ë§Œ
model = MultiModalModel("llava-hf/llava-1.5-7b-hf")
for scene in results['scenes']:
    scene['description'] = model.generate_from_image(scene['frame_path'])
```

---

## ğŸ“ ìš”ì•½

### ì™„ì „í•œ íŒŒì´í”„ë¼ì¸

```bash
# 1ë‹¨ê³„: ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirements_video.txt
pip install -r requirements_multimodal.txt
brew install ffmpeg

# 2ë‹¨ê³„: ë™ì˜ìƒ ë¶„ì„
python src/video_analyzer.py video.mp4 --use_multimodal

# 3ë‹¨ê³„: ê²°ê³¼ í™•ì¸
cat outputs/video_analysis/editing_guide.txt

# 4ë‹¨ê³„: í¸ì§‘ ì†Œí”„íŠ¸ì›¨ì–´ì—ì„œ í™œìš©
# editing_data.json ì‚¬ìš©
```

### í•µì‹¬ ê¸°ëŠ¥

âœ… **ffmpeg**: ë™ì˜ìƒ â†’ ì´ë¯¸ì§€ + ì˜¤ë””ì˜¤
âœ… **Whisper**: ìŒì„± â†’ í…ìŠ¤íŠ¸ (íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨)
âœ… **ë©€í‹°ëª¨ë‹¬ LLM**: ì´ë¯¸ì§€ ë¶„ì„
âœ… **í†µí•© ë¶„ì„**: ì‹œê° + ì²­ê° ê²°í•©
âœ… **í¸ì§‘ ë©”íƒ€ë°ì´í„°**: ìë™ ìƒì„±

**ë™ì˜ìƒ í¸ì§‘ì´ í›¨ì”¬ ì‰¬ì›Œì§‘ë‹ˆë‹¤!** ğŸ¬âœ¨

