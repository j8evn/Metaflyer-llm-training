# ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ íŒŒì¸íŠœë‹ ê°€ì´ë“œ

ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ë¥¼ í•¨ê»˜ ì‚¬ìš©í•˜ëŠ” Vision-Language ëª¨ë¸ íŒŒì¸íŠœë‹ ê°€ì´ë“œìž…ë‹ˆë‹¤.

## ëª©ì°¨
1. [ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ì´ëž€?](#ë©€í‹°ëª¨ë‹¬-ëª¨ë¸ì´ëž€)
2. [ì§€ì› ëª¨ë¸](#ì§€ì›-ëª¨ë¸)
3. [ë°ì´í„° ì¤€ë¹„](#ë°ì´í„°-ì¤€ë¹„)
4. [í•™ìŠµ ì‹¤í–‰](#í•™ìŠµ-ì‹¤í–‰)
5. [ì¶”ë¡  ë° ì‚¬ìš©](#ì¶”ë¡ -ë°-ì‚¬ìš©)

---

## ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ì´ëž€?

**Vision-Language ëª¨ë¸**ì€ ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ë¥¼ í•¨ê»˜ ì´í•´í•˜ëŠ” AI ëª¨ë¸ìž…ë‹ˆë‹¤.

### ì£¼ìš” ê¸°ëŠ¥

- ðŸ“¸ **ì´ë¯¸ì§€ ì„¤ëª…** (Image Captioning)
- â“ **ì´ë¯¸ì§€ Q&A** (Visual Question Answering)
- ðŸ” **ì´ë¯¸ì§€ ê¸°ë°˜ ëŒ€í™”**
- ðŸ“Š **ì°¨íŠ¸/ê·¸ëž˜í”„ ë¶„ì„**
- ðŸ“ **OCR ë° ë¬¸ì„œ ì´í•´**

### ì‚¬ìš© ì‚¬ë¡€

- ì˜ë£Œ ì˜ìƒ ë¶„ì„
- ì œí’ˆ ì´ë¯¸ì§€ ì„¤ëª… ìƒì„±
- ì‹œê°ì  ì½˜í…ì¸  ê²€ìƒ‰
- ìžë™ ì´ë¯¸ì§€ íƒœê¹…
- ì‹œê° ìž¥ì• ì¸ ë³´ì¡°

---

## ì§€ì› ëª¨ë¸

### LLaVA (Large Language and Vision Assistant)

**ê°€ìž¥ ì¸ê¸° ìžˆëŠ” ì˜¤í”ˆ ì†ŒìŠ¤ Vision-Language ëª¨ë¸**

```bash
# LLaVA 1.5 7B
model_name: "llava-hf/llava-1.5-7b-hf"

# LLaVA 1.5 13B
model_name: "llava-hf/llava-1.5-13b-hf"

# LLaVA 1.6 (Mistral ê¸°ë°˜)
model_name: "llava-hf/llava-v1.6-mistral-7b-hf"
```

**íŠ¹ì§•:**
- âœ… GPT-4Vì™€ ìœ ì‚¬í•œ ì„±ëŠ¥
- âœ… ì˜¤í”ˆ ì†ŒìŠ¤
- âœ… íŒŒì¸íŠœë‹ ìš©ì´

### BLIP-2

```bash
# BLIP-2 OPT 2.7B
model_name: "Salesforce/blip2-opt-2.7b"

# BLIP-2 Flan-T5 XL
model_name: "Salesforce/blip2-flan-t5-xl"
```

**íŠ¹ì§•:**
- âœ… íš¨ìœ¨ì ì¸ í•™ìŠµ
- âœ… ë‹¤ì–‘í•œ ë°±ë³¸ ì§€ì›
- âœ… ë¹ ë¥¸ ì¶”ë¡ 

### InstructBLIP

```bash
model_name: "Salesforce/instructblip-vicuna-7b"
```

**íŠ¹ì§•:**
- âœ… Instruction following
- âœ… ë³µìž¡í•œ ì§ˆë¬¸ ì²˜ë¦¬

---

## ë°ì´í„° ì¤€ë¹„

### ë°ì´í„° í˜•ì‹

ë©€í‹°ëª¨ë‹¬ í•™ìŠµ ë°ì´í„°ëŠ” **ì´ë¯¸ì§€ íŒŒì¼ + JSON ë©”íƒ€ë°ì´í„°** í˜•ì‹ìž…ë‹ˆë‹¤.

#### í˜•ì‹ 1: ì´ë¯¸ì§€ ì„¤ëª… (Captioning)

```json
[
    {
        "image": "data/images/cat.jpg",
        "text": "ê³ ì–‘ì´ê°€ ì†ŒíŒŒì— íŽ¸ì•ˆí•˜ê²Œ ì•‰ì•„ ìžˆìŠµë‹ˆë‹¤."
    },
    {
        "image": "data/images/dog.jpg",
        "text": "ê³¨ë“  ë¦¬íŠ¸ë¦¬ë²„ ê°•ì•„ì§€ê°€ ê³µì›ì—ì„œ ë›°ì–´ë†€ê³  ìžˆìŠµë‹ˆë‹¤."
    }
]
```

#### í˜•ì‹ 2: Visual Q&A

```json
[
    {
        "image": "data/images/food.jpg",
        "question": "ì´ ìŒì‹ì€ ë¬´ì—‡ì¸ê°€ìš”?",
        "answer": "ë§›ìžˆì–´ ë³´ì´ëŠ” í”¼ìžìž…ë‹ˆë‹¤. í† í•‘ìœ¼ë¡œ ì¹˜ì¦ˆ, í† ë§ˆí† , ë°”ì§ˆì´ ì˜¬ë¼ê°€ ìžˆìŠµë‹ˆë‹¤."
    },
    {
        "image": "data/images/chart.jpg",
        "question": "ì´ ê·¸ëž˜í”„ì—ì„œ ê°€ìž¥ ë†’ì€ ê°’ì€?",
        "answer": "2023ë…„ 3ì›”ì´ ê°€ìž¥ ë†’ì€ ê°’ìœ¼ë¡œ ì•½ 150ì„ ë‚˜íƒ€ë‚´ê³  ìžˆìŠµë‹ˆë‹¤."
    }
]
```

#### í˜•ì‹ 3: Instruction í˜•ì‹

```json
[
    {
        "image": "data/images/product.jpg",
        "instruction": "ì´ ì œí’ˆì˜ íŠ¹ì§•ì„ ì„¤ëª…í•˜ì„¸ìš”",
        "input": "",
        "output": "ì´ ì œí’ˆì€ ê³ ê¸‰ ê°€ì£½ìœ¼ë¡œ ë§Œë“¤ì–´ì§„ ê²€ì€ìƒ‰ ì§€ê°‘ìž…ë‹ˆë‹¤. ì„¸ë ¨ëœ ë””ìžì¸ê³¼ ì‹¤ìš©ì ì¸ ì¹´ë“œ ìŠ¬ë¡¯ì´ íŠ¹ì§•ìž…ë‹ˆë‹¤."
    }
]
```

### ë””ë ‰í† ë¦¬ êµ¬ì¡°

```
data/
â”œâ”€â”€ images/              # ì´ë¯¸ì§€ íŒŒì¼ë“¤
â”‚   â”œâ”€â”€ cat.jpg
â”‚   â”œâ”€â”€ dog.jpg
â”‚   â”œâ”€â”€ food.jpg
â”‚   â””â”€â”€ ...
â”œâ”€â”€ multimodal_train.json  # í•™ìŠµ ë©”íƒ€ë°ì´í„°
â””â”€â”€ multimodal_eval.json   # í‰ê°€ ë©”íƒ€ë°ì´í„°
```

### ìƒ˜í”Œ ë°ì´í„° ìƒì„±

```python
# ìƒ˜í”Œ JSON ìƒì„±
python -c "from src.multimodal_utils import create_sample_multimodal_dataset; \
create_sample_multimodal_dataset('data/multimodal_train.json', 20)"
```

**ì£¼ì˜:** ì‹¤ì œ ì´ë¯¸ì§€ íŒŒì¼ì„ `data/images/` ë””ë ‰í† ë¦¬ì— ì¤€ë¹„í•´ì•¼ í•©ë‹ˆë‹¤!

---

## í•™ìŠµ ì‹¤í–‰

### ê¸°ë³¸ í•™ìŠµ

```bash
python src/train_multimodal.py \
    --model_name "llava-hf/llava-1.5-7b-hf" \
    --model_type "llava" \
    --dataset_path "data/multimodal_train.json" \
    --output_dir "outputs/llava_finetuned"
```

### ì„¤ì • íŒŒì¼ ì‚¬ìš©

```bash
python src/train_multimodal.py --config configs/multimodal_config.yaml
```

### LoRAë¥¼ ì‚¬ìš©í•œ íš¨ìœ¨ì  í•™ìŠµ

```bash
# configs/multimodal_config.yaml
lora:
  use_lora: true
  r: 16
  lora_alpha: 32

training:
  batch_size: 2
  gradient_accumulation_steps: 8
```

ì‹¤í–‰:
```bash
python src/train_multimodal.py --config configs/multimodal_config.yaml
```

---

## ì¶”ë¡  ë° ì‚¬ìš©

### 1. ì´ë¯¸ì§€ ì„¤ëª… ìƒì„±

```bash
python src/inference_multimodal.py \
    --model_path "outputs/llava_finetuned/final_model" \
    --model_type "llava" \
    --image "test_image.jpg"
```

### 2. ì´ë¯¸ì§€ì— ëŒ€í•œ ì§ˆë¬¸

```bash
python src/inference_multimodal.py \
    --model_path "outputs/llava_finetuned/final_model" \
    --model_type "llava" \
    --image "test_image.jpg" \
    --question "ì´ ì´ë¯¸ì§€ì—ì„œ ë¬´ì—‡ì„ ë³¼ ìˆ˜ ìžˆë‚˜ìš”?"
```

### 3. ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸

```bash
python src/inference_multimodal.py \
    --model_path "outputs/llava_finetuned/final_model" \
    --model_type "llava" \
    --image "test_image.jpg" \
    --prompt "ì´ ì´ë¯¸ì§€ì˜ ìƒ‰ìƒ êµ¬ì„±ì„ ë¶„ì„í•˜ì„¸ìš”"
```

### 4. ëŒ€í™”í˜• ëª¨ë“œ

```bash
python src/inference_multimodal.py \
    --model_path "outputs/llava_finetuned/final_model" \
    --model_type "llava"
```

ëŒ€í™”í˜• ì¸í„°íŽ˜ì´ìŠ¤:
```
> describe data/images/cat.jpg
ì„¤ëª…:
ê·€ì—¬ìš´ ê³ ì–‘ì´ê°€ ì†ŒíŒŒì—ì„œ ë‚®ìž ì„ ìžê³  ìžˆìŠµë‹ˆë‹¤...

> ask data/images/food.jpg ì´ ìŒì‹ì€ ë¬´ì—‡ì¸ê°€ìš”?
ë‹µë³€:
ë§›ìžˆì–´ ë³´ì´ëŠ” í”¼ìžìž…ë‹ˆë‹¤...
```

---

## Python ì½”ë“œë¡œ ì‚¬ìš©

```python
from src.multimodal_utils import MultiModalModel

# ëª¨ë¸ ì´ˆê¸°í™”
model = MultiModalModel(
    model_name="outputs/llava_finetuned/final_model",
    model_type="llava"
)

# ì´ë¯¸ì§€ ì„¤ëª…
description = model.generate_from_image(
    image_path="test_image.jpg",
    prompt="ì´ ì´ë¯¸ì§€ë¥¼ ìžì„¸ížˆ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
    max_new_tokens=256
)

print(description)
```

---

## ì‹¤ì „ ì˜ˆì œ

### ì˜ˆì œ 1: ì˜ë£Œ ì˜ìƒ ë¶„ì„

```json
// data/medical_train.json
[
    {
        "image": "data/images/xray_001.jpg",
        "question": "ì´ X-ray ì´ë¯¸ì§€ì—ì„œ ì´ìƒ ì†Œê²¬ì´ ìžˆë‚˜ìš”?",
        "answer": "ì¢Œì¸¡ í í•˜ë¶€ì— ì•½ê°„ì˜ ìŒì˜ì´ ê´€ì°°ë©ë‹ˆë‹¤. ì¶”ê°€ ê²€ì‚¬ê°€ í•„ìš”í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤."
    }
]
```

í•™ìŠµ:
```bash
python src/train_multimodal.py \
    --model_name "llava-hf/llava-1.5-7b-hf" \
    --dataset_path "data/medical_train.json" \
    --output_dir "outputs/medical_assistant"
```

### ì˜ˆì œ 2: ì œí’ˆ ì„¤ëª… ìƒì„±

```json
// data/product_train.json
[
    {
        "image": "data/images/product_001.jpg",
        "instruction": "ì´ ì œí’ˆì˜ íŒë§¤ ë¬¸êµ¬ë¥¼ ìž‘ì„±í•˜ì„¸ìš”",
        "output": "í”„ë¦¬ë¯¸ì—„ ê°€ì£½ ì§€ê°‘ - ì„¸ë ¨ëœ ë””ìžì¸ê³¼ ë›°ì–´ë‚œ ë‚´êµ¬ì„±ì„ ìžëž‘í•˜ëŠ” ìµœê³ ê¸‰ ì œí’ˆìž…ë‹ˆë‹¤. 12ê°œì˜ ì¹´ë“œ ìŠ¬ë¡¯ê³¼ ë„‰ë„‰í•œ ì§€í ê³µê°„ì„ ì œê³µí•©ë‹ˆë‹¤."
    }
]
```

### ì˜ˆì œ 3: ì°¨íŠ¸ ë¶„ì„

```json
// data/chart_train.json
[
    {
        "image": "data/images/sales_chart.jpg",
        "question": "ì´ íŒë§¤ ê·¸ëž˜í”„ì˜ ì£¼ìš” íŠ¸ë Œë“œëŠ”?",
        "answer": "2023ë…„ 1ì›”ë¶€í„° 6ì›”ê¹Œì§€ ë§¤ì¶œì´ ê¾¸ì¤€ížˆ ìƒìŠ¹í•˜ëŠ” ì¶”ì„¸ë¥¼ ë³´ì´ê³  ìžˆìœ¼ë©°, íŠ¹ížˆ 3ì›”ì— ê¸‰ê²©í•œ ì¦ê°€ê°€ ìžˆì—ˆìŠµë‹ˆë‹¤."
    }
]
```

---

## ë©”ëª¨ë¦¬ ìµœì í™”

ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ì€ ë©”ëª¨ë¦¬ë¥¼ ë§Žì´ ì‚¬ìš©í•©ë‹ˆë‹¤. ìµœì í™” íŒ:

### 1. LoRA ì‚¬ìš© (í•„ìˆ˜)

```yaml
lora:
  use_lora: true
  r: 16
```

### 2. ìž‘ì€ ë°°ì¹˜ í¬ê¸°

```yaml
training:
  batch_size: 1
  gradient_accumulation_steps: 16
```

### 3. ê·¸ëž˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ…

```yaml
advanced:
  gradient_checkpointing: true
```

### 4. ì´ë¯¸ì§€ í¬ê¸° ì¡°ì •

```yaml
image_processing:
  resize: [224, 224]  # ìž‘ê²Œ ì„¤ì •
```

---

## ì§€ì› ëª¨ë¸ ìƒì„¸

### LLaVA ê³„ì—´

| ëª¨ë¸ | í¬ê¸° | GPU ë©”ëª¨ë¦¬ (LoRA) | íŠ¹ì§• |
|------|------|-------------------|------|
| llava-1.5-7b-hf | 7B | 16GB | ê· í˜•ìž¡ížŒ |
| llava-1.5-13b-hf | 13B | 24GB | ê³ ì„±ëŠ¥ |
| llava-v1.6-mistral-7b | 7B | 16GB | Mistral ê¸°ë°˜ |

### BLIP ê³„ì—´

| ëª¨ë¸ | í¬ê¸° | GPU ë©”ëª¨ë¦¬ | íŠ¹ì§• |
|------|------|------------|------|
| blip2-opt-2.7b | 2.7B | 8GB | ê°€ë²¼ì›€ |
| blip2-flan-t5-xl | 3B | 12GB | T5 ê¸°ë°˜ |

---

## ì „ì²´ ì›Œí¬í”Œë¡œìš°

```bash
# 1. ì¶”ê°€ ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirements_multimodal.txt

# 2. ì´ë¯¸ì§€ ë°ì´í„° ì¤€ë¹„
mkdir -p data/images
# ì´ë¯¸ì§€ íŒŒì¼ë“¤ì„ data/images/ì— ë³µì‚¬

# 3. JSON ë©”íƒ€ë°ì´í„° ìƒì„±
cat > data/multimodal_train.json << 'JSON'
[
    {
        "image": "data/images/image1.jpg",
        "question": "ì´ ì´ë¯¸ì§€ë¥¼ ì„¤ëª…í•˜ì„¸ìš”",
        "answer": "ìƒì„¸í•œ ì„¤ëª…..."
    }
]
JSON

# 4. í•™ìŠµ
python src/train_multimodal.py --config configs/multimodal_config.yaml

# 5. ì¶”ë¡ 
python src/inference_multimodal.py \
    --model_path "outputs/multimodal_checkpoints/final_model" \
    --model_type "llava" \
    --image "test.jpg"
```

---

## ë°ì´í„°ì…‹ ì˜ˆì œ

### ê³µê°œ ë°ì´í„°ì…‹ ì‚¬ìš©

#### COCO Captions

```python
from datasets import load_dataset

# COCO ë°ì´í„°ì…‹ ë¡œë”©
dataset = load_dataset("HuggingFaceM4/COCO")

# ë³€í™˜
multimodal_data = []
for item in dataset['train']:
    multimodal_data.append({
        "image": item['image'],  # PIL Image
        "text": item['sentences'][0]['raw']
    })
```

#### VQA (Visual Question Answering)

```python
from datasets import load_dataset

dataset = load_dataset("HuggingFaceM4/VQAv2")

for item in dataset['train']:
    multimodal_data.append({
        "image": item['image'],
        "question": item['question'],
        "answer": item['multiple_choice_answer']
    })
```

---

## Python ì‚¬ìš© ì˜ˆì œ

### í•™ìŠµëœ ëª¨ë¸ë¡œ ì¶”ë¡ 

```python
from src.inference_multimodal import MultiModalInference

# ì—”ì§„ ì´ˆê¸°í™”
engine = MultiModalInference(
    model_path="outputs/llava_finetuned/final_model",
    model_type="llava"
)

# ì´ë¯¸ì§€ ì„¤ëª…
description = engine.describe_image("cat.jpg")
print(f"ì„¤ëª…: {description}")

# ì´ë¯¸ì§€ Q&A
answer = engine.answer_question(
    image_path="chart.jpg",
    question="ì´ ì°¨íŠ¸ì˜ íŠ¸ë Œë“œëŠ”?"
)
print(f"ë‹µë³€: {answer}")

# ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸
result = engine.generate(
    image_path="product.jpg",
    prompt="ì´ ì œí’ˆì˜ ìž¥ë‹¨ì ì„ ë¶„ì„í•˜ì„¸ìš”",
    max_new_tokens=300
)
print(f"ë¶„ì„: {result}")
```

---

## API ì„œë²„ì— í†µí•©

### ë©€í‹°ëª¨ë‹¬ API ì—”ë“œí¬ì¸íŠ¸ ì¶”ê°€

```python
# src/api_server.pyì— ì¶”ê°€

from fastapi import File, UploadFile
from src.multimodal_utils import MultiModalModel
import shutil

# ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ ì´ˆê¸°í™”
multimodal_model = None

@app.post("/multimodal/describe")
async def describe_image(file: UploadFile = File(...)):
    """ì´ë¯¸ì§€ ì„¤ëª… ìƒì„±"""
    
    # ìž„ì‹œ íŒŒì¼ ì €ìž¥
    temp_path = f"temp_{file.filename}"
    with open(temp_path, "wb") as buffer:
        shutil.copyfileobj(file.file, buffer)
    
    try:
        # ì„¤ëª… ìƒì„±
        description = multimodal_model.generate_from_image(
            temp_path,
            "ì´ ì´ë¯¸ì§€ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”."
        )
        
        return {"description": description}
    
    finally:
        # ìž„ì‹œ íŒŒì¼ ì‚­ì œ
        os.remove(temp_path)


@app.post("/multimodal/vqa")
async def visual_qa(
    file: UploadFile = File(...),
    question: str = ""
):
    """Visual Question Answering"""
    
    temp_path = f"temp_{file.filename}"
    with open(temp_path, "wb") as buffer:
        shutil.copyfileobj(file.file, buffer)
    
    try:
        answer = multimodal_model.generate_from_image(
            temp_path,
            f"ì§ˆë¬¸: {question}\në‹µë³€:"
        )
        
        return {
            "question": question,
            "answer": answer
        }
    
    finally:
        os.remove(temp_path)
```

ì‚¬ìš©:
```bash
# ì´ë¯¸ì§€ ì„¤ëª…
curl -X POST http://localhost:8000/multimodal/describe \
  -F "file=@cat.jpg"

# Visual Q&A
curl -X POST "http://localhost:8000/multimodal/vqa?question=ë¬´ì—‡ì´ë³´ì´ë‚˜ìš”" \
  -F "file=@image.jpg"
```

---

## ë² ìŠ¤íŠ¸ í”„ëž™í‹°ìŠ¤

### 1. ë°ì´í„° í’ˆì§ˆ

âœ… **ì¢‹ì€ ë°ì´í„°:**
- ê³ í•´ìƒë„ ì´ë¯¸ì§€ (ìµœì†Œ 224x224)
- ìƒì„¸í•˜ê³  ì •í™•í•œ ì„¤ëª…
- ë‹¤ì–‘í•œ ê°ë„ì™€ ì¡°ëª…

âŒ **í”¼í•´ì•¼ í•  ê²ƒ:**
- íë¦¿í•˜ê±°ë‚˜ ì €í™”ì§ˆ ì´ë¯¸ì§€
- ë¶€ì •í™•í•œ ì„¤ëª…
- íŽ¸í–¥ëœ ë°ì´í„°

### 2. í•˜ì´í¼íŒŒë¼ë¯¸í„°

```yaml
# ì¶”ì²œ ì„¤ì •
training:
  batch_size: 2
  gradient_accumulation_steps: 8
  learning_rate: 2.0e-5

lora:
  r: 16  # Vision-LanguageëŠ” 16 ì¶”ì²œ
  lora_alpha: 32
```

### 3. í‰ê°€

```python
# ì •ì„± í‰ê°€
from src.inference_multimodal import MultiModalInference

engine = MultiModalInference("outputs/model", "llava")

test_images = ["test1.jpg", "test2.jpg", "test3.jpg"]

for img in test_images:
    desc = engine.describe_image(img)
    print(f"ì´ë¯¸ì§€: {img}")
    print(f"ì„¤ëª…: {desc}\n")
```

---

## íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### ë¬¸ì œ 1: GPU ë©”ëª¨ë¦¬ ë¶€ì¡±

```yaml
# í•´ê²°ì±…
training:
  batch_size: 1
  gradient_accumulation_steps: 16

lora:
  use_lora: true

quantization:
  use_quantization: true
  bits: 4
```

### ë¬¸ì œ 2: ì´ë¯¸ì§€ ë¡œë”© ì‹¤íŒ¨

```python
# ì´ë¯¸ì§€ ê²½ë¡œ í™•ì¸
import os
print(os.path.exists("data/images/cat.jpg"))

# PIL ì´ë¯¸ì§€ í…ŒìŠ¤íŠ¸
from PIL import Image
img = Image.open("data/images/cat.jpg")
img.show()
```

### ë¬¸ì œ 3: í•™ìŠµ ì†ë„ ëŠë¦¼

- ì´ë¯¸ì§€ í¬ê¸° ì¤„ì´ê¸° (224x224)
- ë°°ì¹˜ í¬ê¸° ì¡°ì •
- ë°ì´í„° ë¡œë” ì›Œì»¤ ìˆ˜ ì¦ê°€

---

## ê³ ê¸‰: ì»¤ìŠ¤í…€ Vision Encoder

ìžì‹ ë§Œì˜ vision encoder ì‚¬ìš©:

```python
from transformers import CLIPVisionModel

# CLIP vision encoder
vision_model = CLIPVisionModel.from_pretrained("openai/clip-vit-large-patch14")

# LLMê³¼ ê²°í•©
# (ê³ ê¸‰ ì‚¬ìš©ìžìš© - ì•„í‚¤í…ì²˜ ìˆ˜ì • í•„ìš”)
```

---

## ìš”ì•½

### ë¹ ë¥¸ ì‹œìž‘

```bash
# 1. ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirements_multimodal.txt

# 2. ë°ì´í„° ì¤€ë¹„
# data/images/ ì— ì´ë¯¸ì§€ íŒŒì¼
# data/multimodal_train.json ì— ë©”íƒ€ë°ì´í„°

# 3. í•™ìŠµ
python src/train_multimodal.py --config configs/multimodal_config.yaml

# 4. ì¶”ë¡ 
python src/inference_multimodal.py \
    --model_path "outputs/multimodal_checkpoints/final_model" \
    --image "test.jpg"
```

### ì§€ì› ëª¨ë¸

- âœ… LLaVA (ê¶Œìž¥)
- âœ… BLIP-2
- âœ… InstructBLIP

### ë°ì´í„° í˜•ì‹

- Image Captioning: image + text
- Visual Q&A: image + question + answer
- Instruction: image + instruction + output

**ë©€í‹°ëª¨ë‹¬ AIì˜ ì„¸ê³„ì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤!** ðŸŽ¨ðŸ¤–


